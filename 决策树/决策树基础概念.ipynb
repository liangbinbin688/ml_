{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.从LR到决策树\n",
    "\n",
    "        LR模型是利用线性回归的预测值，通过sigmoid映射为概率，来对数据做预测，有非常友好的数据预处理特性，工业界应用很丰富。\n",
    "        决策树是（decision tree model）是一个模拟人类决策过程思想的模型。\n",
    "        \n",
    "# 2.分类树\n",
    "\n",
    "        决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。\n",
    "       1.开始：构建根节点，将所有训练数据都放在根节点，选择一个最优特征，按着这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。\n",
    "       \n",
    "       2.如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。\n",
    "       \n",
    "       3.如果还有子集不能够被正确的分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点，如果递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止。\n",
    "       \n",
    "       4.每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。\n",
    "       \n",
    "## 2.1决策树的关键因素\n",
    "\n",
    "        决策树的构造方式：基于树的结构进行决策\n",
    "        \n",
    "        + 内部节点：对应对属性的测试\n",
    "        \n",
    "        + 分支：对应属性可能的结果\n",
    "        \n",
    "        + 叶节点：对应一个预测结果\n",
    "        \n",
    "        *****\n",
    "   **决策树的学习过程（训练）**：通过对训练样本的分析，来确定划分的属性。\n",
    "   \n",
    "   **决策树的预测过程**： 将示例从根节点开始，沿着划分属性进行划分，直到叶节点\n",
    "   \n",
    "   **决策树通常有三个步骤**：特征选择、决策树的生成、决策树的修剪。\n",
    "   \n",
    "   **决策树学习的目标**：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。\n",
    "   \n",
    "   **决策树学习的本质**：从训练集中归纳出一组分类规则，或者说是***由训练数据集估计条件概率模型***。\n",
    "   \n",
    "   **决策树学习的损失函数**：正则化的极大似然函数\n",
    "   \n",
    "   **决策树学习的测试**：最小化损失函数\n",
    "   \n",
    "   **决策树学习的目标**：在损失函数的意义下，选择最优决策树的问题。\n",
    "   \n",
    "   ***决策树算法的核心：从属性集中选择最优的划分属性***\n",
    "   \n",
    "  ***决策树的特点**：\n",
    "  \n",
    "    -优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。\n",
    "    \n",
    "    -缺点：可能会产生过度匹配的问题\n",
    "    \n",
    "    -适用数据类型：数值型和标称型\n",
    "    \n",
    "    首先：确定当前数据集上的决定性特征，为了得到该决定性特征，必须评估每个特征，完成测试之后，原始数据集就被划分为几个数据子集，这些数据子集会分布在第一个决策点的所有分支上，如果某个分支下的数据属于同一类型，则当前无序阅读的垃圾邮件已经正确的划分数据分类，无需进一步对数据集进行分割，如果不属于同一类，则要重复划分数据子集，直到所有相同类型的数据均在一个数据子集内。\n",
    "   \n",
    "## 2.2如何选择最优的划分属性\n",
    "\n",
    "### 2.2.1熵\n",
    "\n",
    "        熵是度量样本不纯度的指标，样本不纯度越大，熵越大，越无序。\n",
    "        \n",
    "        熵是对平均不决定性的度量。\n",
    "        \n",
    "        熵是度量信息的期望。概率越大，信息越小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***信息的公式为：***\n",
    "$$l(x_i)=-log_2p(x_i)$$\n",
    "***熵：***\n",
    "$$H=-\\sum_{i=1}^{n}p(x_i)log_2p(x_i)$$\n",
    "***熵的公式***\n",
    "$$Ent(D)=-\\sum_{k=1}^{|y|}p_klog_2p_k$$\n",
    "\n",
    "熵越小，样本的纯度越高，所以决策树的生长过程也是不断的将数据的不纯度降低的过程，希望最后得到的分类结果纯的很高，也就是准确性很高。\n",
    "\n",
    "### 2.2.2 信息增益：ID3\n",
    "\n",
    "机器学习的过程是希望将样本的不纯度往下降的过程，那么最优属性的划分可以参考那个属性对当前样本的熵降低的最多。\n",
    "\n",
    "离散属性a的取值是${a^1,a^2,..,a^V}$,\n",
    "$D^V$:$D$中在a上取值=$a^1,a^2,...a^v$的样本集合\n",
    "以属性$a$对数据集$D$进行划分所获得的信息增益为：\n",
    "\n",
    "$$Gain(D,a)=Ent(D)-\\sum_{v=1}^{V}{\\frac{|D|}{|D^v|}Ent(D^v)}$$\n",
    "\n",
    "其中：$Ent(D)$是划分前的信息熵，后部分是划分后的信息熵。\n",
    "$\\frac{|D|}{|D^v|}$表示第$v$个分支的权重，样本越多越重要。\n",
    "\n",
    "**信息增益实例;**\n",
    "    该数据集包括17个训练样本，结果有2个类别，$|y|=2$,其中正例占$p_1=\\frac{8}{17}$,反例占$p_2=\\frac{9}{17}$,根节点的信息熵为：$$Ent(D)=-\\sum_{k=1}^{2}p_klog_2p_k=-(\\frac{8}{17}log_2{\\frac{8}{17}}+{\\frac{9}{17}}log_2{\\frac{9}{17}})$$\n",
    "    以属性“色泽”为例，对应的3个属性子集是$D^1$,$D^2$,$D^3$.\n",
    "    其中，$D^1$有六个样本，正例占$p_1={\\frac{3}{6}}$,反例占$p_2={\\frac{3}{6}}$,$D^2$,$D^3$同理，3个节点的信息熵为：$$Ent(D^1)=-({\\frac{3}{6}log_2{\\frac{3}{6}}}{\\frac{3}{6}log_2{\\frac{3}{6}}})=1.00$$\n",
    "    同理可得：\n",
    "    $$Ent(D^2)=0.918$$\n",
    "    $$Ent(D^3)=0.722$$\n",
    "    \n",
    "    $$Gain(D,色泽)=Ent(D)-{\\sum_{v=1}^{3}\\frac{|D^v|}{|D|}Ent(D^v)}$$\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "属性色泽的信息增益为;\n",
    "$$Gain(D,色泽)=Ent(D)-{\\sum_{v=1}^{3}\\frac{|D^v|}{|D|}Ent(D^v)}=0.109$$\n",
    "同样的方法判断其他属性的信息增益，比较大的作为当前的划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 信息增益率（C4.5）\n",
    "     信息增益存在的问题（对可取值数目较多的属性有所偏好）：假如我们用“学号”作为特征，也就是每个人一个学号，将45个人分到45个桶里，此时的纯度最高，但是对新来的特征没有任何意义，即分叉越多，越肯定，纯度越高，所带来的增益很大，所以我们选用相对值来衡量信息的增益，也就是信息增益率。\n",
    "     信息增益率：\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 其中:\n",
    "$$IV(a)=-\\sum_{v=1}^{V}{\\frac{|D^v|}{|D|}}log_2{\\frac{|D^v|}{|D|}}$$\n",
    "某属性分的种类越多，$IV(a)$的值通常越大；\n",
    "分叉越多，熵越大，所以不能只看增益，还要参考该属性自身的熵值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4CART树(二叉树)\n",
    "\n",
    "CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。\n",
    "        \n",
    "1.CART 的全称是 分类树与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。\n",
    "        \n",
    "回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。\n",
    "        \n",
    "要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。\n",
    "        \n",
    "2.分类树，使用**Gini 指数**最小化准则来选择特征并进行划分；\n",
    "        \n",
    "Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。\n",
    "\n",
    "***Gini 指数 vs 熵***\n",
    "        \n",
    "    既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？\n",
    "    \n",
    "-Gini 指数的计算不需要对数运算，更加高效；\n",
    "\n",
    "-Gini 指数更偏向于连续属性，熵更偏向于离散属性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼指数（gini index）:Cart中使用：\n",
    "    $$Gini(D)={\\sum_{k=1}^{|y|}}{\\sum_{k^`！=k}}p_kp_{k^`}$$\n",
    "    $$=1-{\\sum_{k=1}^{|y|}{p_k}^2}$$\n",
    "    \n",
    "**反应了从D中随机抽取两个样本，其类别标签不一致的概率**\n",
    "\n",
    "Gini(D)越小，数据集D的纯度越高。\n",
    "\n",
    "属性a 的基尼指数：\n",
    "\n",
    "$$Gini_iindex(D,a)=\\sum_{v=1}^{V}{\\frac{|D_v|}{|D|}}Gini(D^v)$$\n",
    "\n",
    "属性a按照一定规则划分成v个样本集，求各个样本集的基尼指数，按照比例进行求和，在候选属性集合中，选取那个是划分基尼指数最小的属性。\n",
    "\n",
    "![title](images/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 三种决策树的比较\n",
    "\n",
    "1.ID3:\n",
    "\n",
    "    取值多的属性，更容易 使数据更纯，其信息增益更大，训练得到的是一颗庞大且深度 浅的树，不合理；\n",
    "    \n",
    "    熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。\n",
    "     信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。\n",
    "\n",
    "      ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。\n",
    "    \n",
    "2.C4.5：\n",
    "      \n",
    "      采用信息增益率代替信息增益。\n",
    "      \n",
    "      C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。\n",
    "      \n",
    "      C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。但是，对连续属性值需要扫描排序，会使C4.5性能下降。\n",
    "\n",
    "    ID3和C4.5都是分类树，CART（Classification and regression tree）可以是分类/回归树。\n",
    "     \n",
    "3.CRAT\n",
    "\n",
    "    基尼指数代替熵，最小化不纯度，不是最大化信息增益。\n",
    "    \n",
    "    CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。\n",
    "\n",
    "**这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用信息增益作为选择特征的准则；C4.5 使用信息增益比作为选择特征的准则；CART 使用 Gini 指数作为选择特征的准则。**\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4构建决策树\n",
    "\n",
    "**使用决策树做预测需要以下过程：**\n",
    "\n",
    "1.收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过参访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。\n",
    "\n",
    "2.准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。\n",
    "\n",
    "3.分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。\n",
    "\n",
    "4.训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。\n",
    "\n",
    "5.测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。\n",
    "\n",
    "6.使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。\n",
    "\n",
    "### 2.4.1决策树的生成和修剪\n",
    "\n",
    "我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。  \n",
    "\n",
    "构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。\n",
    "\n",
    "决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。\n",
    "\n",
    "**递归构建决策树**\n",
    "\n",
    "从数据集构造决策树算法所需的子功能模块工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分，第一次划分之后，数据将被向下传递到树分支的下一个节点，在此节点在此划分数据，因此可以使用递归的原则处理数据集。\n",
    "\n",
    "**递归结束的条件是:**\n",
    "\n",
    "程序完全遍历所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类，如果所有实例具有相同的分类，则得到一个叶子节点或者终止块，任何到达叶子节点的数据必然属于叶子节点的分类。\n",
    "###  2.4.2 决策树的剪枝\n",
    "\n",
    "    决策树生成算法递归的产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确，但对未知测试数据的分类缺没有那么精确，即会出现过拟合现象。过拟合产生的原因在于在学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决方法是考虑决策树的复杂度，对已经生成的树进行简化。\n",
    "    \n",
    "    剪枝（pruning）：从已经生成的树上裁掉一些子树或叶节点，并将其根节点或父节点作为新的叶子节点，从而简化分类树模型。\n",
    "    \n",
    "**实现方法：**\n",
    "        极小化决策树整体的损失函数或代价函数来实现\n",
    "        \n",
    "决策树学习的损失函数定义为：\n",
    "\n",
    "$$C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+{\\alpha}{|T|}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "1.T  表示这棵树的叶子节点\n",
    "\n",
    "2.$H_t(T)$     $$H_t(T)=-{\\sum_{k=1}^{n}{\\frac{N_tk}{N_t}}log{\\frac{N_tk}{N_t}}}$$\n",
    "\n",
    "       表示t个叶子的熵。\n",
    "       n表示叶上的样本个数.\n",
    "       \n",
    "3.$N_t$表示该叶子所含的训练样例的个数.\n",
    "\n",
    "4.{\\alpha}表示惩罚系数\n",
    "\n",
    "5.$|T|$表示子树的叶子节点的个数.\n",
    "\n",
    "其中：\n",
    "\n",
    "$$C(T)=\\sum_{t=1}^{|T|}N_tH_t(T)$$\n",
    "\n",
    "所以有：\n",
    "$$C_{\\alpha}(T)=C(T)+{\\alpha}|T|$$\n",
    "\n",
    "意义：\n",
    "\n",
    "1.$C(T)$  表示模型对训练数据的预测误差，即模型与训练数据的拟合程度。\n",
    "\n",
    "2.$|T|$   表示模型复杂度\n",
    "\n",
    "3.${\\alpha}$  参数${\\alpha}$控制两者之间的误差，较大的${\\alpha}$促使选择简单的模型，较小的${\\alpha}$促使选择较复杂的模型，${\\alpha}=0$意味着只考虑模型和训练数据的拟合程度，不考虑模型的复杂度。\n",
    "\n",
    "损失函数认为对于每个分类终点（叶子节点）的不确定性程度就是分类的损失因子，而叶子节点的个数是模型的复杂程度，作为惩罚项，损失函数的第一项是样本的训练误差，第二项是模型的复杂度。如果一棵子树的损失函数值越大，说明这棵子树越差，因此我们希望让每一棵子树的损失函数值尽可能得小，损失函数最小化就是用正则化的极大似然估计进行模型选择的过程。\n",
    "\n",
    "决策树的剪枝过程（泛化过程）就是从叶子节点开始递归，记其父节点将所有子节点回缩后的子树为$T_b$(分类值取类别比例最大的特征值)，未回缩的子树为$T_a$, 如果$C_{\\alpha}(T_a)>=C_{\\alpha}(T_b)$说明回缩后使得损失函数减少了，那么应该使这棵树回缩，递归直到无法回缩为止，这样使用贪心的思想进行剪枝，可以降低损失函数值，也使决策树得到泛化。\n",
    "\n",
    "决策树的生成只是考虑通过提高信息增益对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。\n",
    "\n",
    "![title](images/002png.png)\n",
    "\n",
    "**树的剪枝算法：**\n",
    "    输入：生成算法产生的整个树T，参数${\\alpha}$\n",
    "    输出：修剪后的子树${T_a}$\n",
    "    \n",
    "    (1)计算每个节点的经验熵\n",
    "    \n",
    "    (2)递归的从树的叶节点向上回缩。\n",
    "    \n",
    " ***总计***\n",
    " ![总计](images/003.png)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3编写代码计算经验熵\n",
    "\n",
    "在编写代码之前，我们先对数据集进行属性标注。\n",
    "\n",
    "年龄：0代表青年，1代表中年，2代表老年；\n",
    "\n",
    "有工作：0代表否，1代表是；\n",
    "\n",
    "有自己的房子：0代表否，1代表是；\n",
    "\n",
    "信贷情况：0代表一般，1代表好，2代表非常好；\n",
    "\n",
    "类别(是否给贷款)：no代表否，yes代表是\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "#创建数据集，计算经验熵：\n",
    "from math import log\n",
    "\n",
    "def createDataSet():\n",
    "    #数据集\n",
    "    dataSet=[[0, 0, 0, 0, 'no'],\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "\n",
    "    labels=['年龄','有工作','有自己的房子','信贷情况']\n",
    "    \n",
    "    return dataSet,labels\n",
    "\n",
    "#函数说明：计算给定数据集的经验熵（香农熵）\n",
    "def calcShannonEnt(dataSet):\n",
    "    #返回数据集行数\n",
    "    numEntries=len(dataSet)\n",
    "    #保存每个标签（label）出现次数的字典\n",
    "    labelCounts={}\n",
    "    #对每组特征向量进行统计\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1]                     #提取标签信息\n",
    "        if currentLabel not in labelCounts.keys():   #如果标签没有放入统计次数的字典，添加进去\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1                 #label计数\n",
    "\n",
    "    shannonEnt=0.0                                   #经验熵\n",
    "    #计算经验熵\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries      #选择该标签的概率\n",
    "        shannonEnt-=prob*log(prob,2)                 #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵\n",
    "#main函数\n",
    "\"\"\"\n",
    "if __name__==\"__main__\":\n",
    "    dataSet,features=createDataSet()\n",
    "    print(calcShannonEnt(dataSet))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 利用代码计算信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "最优索引值：2\n"
     ]
    }
   ],
   "source": [
    "from math import log \n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    #特征数量\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    #计数数据集的香农熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    #信息增益\n",
    "    bestInfoGain = 0.0\n",
    "    #最优特征的索引值\n",
    "    bestFeature = -1\n",
    "    #遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        #创建set集合{}，元素不可重复\n",
    "        uniqueVals = set(featList)\n",
    "        #经验条件熵\n",
    "        newEntropy = 0.0\n",
    "        #计算信息增益\n",
    "        for value in uniqueVals:\n",
    "            #subDataSet划分后的子集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            #计算子集的概率\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            #根据公式计算经验条件熵\n",
    "            newEntropy += prob * calcShannonEnt((subDataSet))\n",
    "        #信息增益\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #打印每个特征的信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))\n",
    "        #计算信息增益\n",
    "        if (infoGain > bestInfoGain):\n",
    "            #更新信息增益，找到最大的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            #记录信息增益最大的特征的索引值\n",
    "            bestFeature = i\n",
    "            #返回信息增益最大特征的索引值\n",
    "    return bestFeature\n",
    "\n",
    "\"\"\"\n",
    "函数说明：按照给定特征划分数据集\n",
    "Parameters：\n",
    "    dataSet：待划分的数据集\n",
    "    axis：划分数据集的特征\n",
    "    value：需要返回的特征的值\n",
    "Returns：\n",
    "    shannonEnt：经验熵\n",
    "\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec=featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\"\"\"\n",
    "if __name__==\"__main__\":\n",
    "    dataSet,features=createDataSet()\n",
    "    print(\"最优索引值：\"+str(chooseBestFeatureToSplit(dataSet)))\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "第0个特征的增益为0.252\n",
      "第1个特征的增益为0.918\n",
      "第2个特征的增益为0.474\n",
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "放贷\n"
     ]
    }
   ],
   "source": [
    "#编写ID3 算法的代码\n",
    "from math import log\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\"\"\"\n",
    "函数说明：计算给定数据集的经验熵（香农熵）\n",
    "Parameters：\n",
    "    dataSet：数据集\n",
    "Returns：\n",
    "    shannonEnt：经验熵\n",
    "Modify：\n",
    "    2018-03-12\n",
    "\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    #返回数据集行数\n",
    "    numEntries=len(dataSet)\n",
    "    #保存每个标签（label）出现次数的字典\n",
    "    labelCounts={}\n",
    "    #对每组特征向量进行统计\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1]                     #提取标签信息\n",
    "        if currentLabel not in labelCounts.keys():   #如果标签没有放入统计次数的字典，添加进去\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1                 #label计数\n",
    "\n",
    "    shannonEnt=0.0                                   #经验熵\n",
    "    #计算经验熵\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries      #选择该标签的概率\n",
    "        shannonEnt-=prob*log(prob,2)                 #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵\n",
    "\n",
    "\"\"\"\n",
    "函数说明：创建测试数据集\n",
    "Parameters：无\n",
    "Returns：\n",
    "    dataSet：数据集\n",
    "    labels：分类属性\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    # 数据集\n",
    "    dataSet=[[0, 0, 0, 0, 'no'],\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    #分类属性\n",
    "    labels=['年龄','有工作','有自己的房子','信贷情况']\n",
    "    #返回数据集和分类属性\n",
    "    return dataSet,labels\n",
    "\n",
    "\"\"\"\n",
    "函数说明：按照给定特征划分数据集\n",
    "\n",
    "Parameters：\n",
    "    dataSet:待划分的数据集\n",
    "    axis：划分数据集的特征\n",
    "    value：需要返回的特征值\n",
    "Returns：\n",
    "    无\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    #创建返回的数据集列表\n",
    "    retDataSet=[]\n",
    "    #遍历数据集\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            #去掉axis特征\n",
    "            reduceFeatVec=featVec[:axis]\n",
    "            #将符合条件的添加到返回的数据集\n",
    "            reduceFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reduceFeatVec)\n",
    "    #返回划分后的数据集\n",
    "    return retDataSet\n",
    "\n",
    "\"\"\"\n",
    "函数说明：计算给定数据集的经验熵（香农熵）\n",
    "Parameters：\n",
    "    dataSet：数据集\n",
    "Returns：\n",
    "    shannonEnt：信息增益最大特征的索引值\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    #特征数量\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    #计数数据集的香农熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    #信息增益\n",
    "    bestInfoGain = 0.0\n",
    "    #最优特征的索引值\n",
    "    bestFeature = -1\n",
    "    #遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        #创建set集合{}，元素不可重复\n",
    "        uniqueVals = set(featList)\n",
    "        #经验条件熵\n",
    "        newEntropy = 0.0\n",
    "        #计算信息增益\n",
    "        for value in uniqueVals:\n",
    "            #subDataSet划分后的子集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            #计算子集的概率\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            #根据公式计算经验条件熵\n",
    "            newEntropy += prob * calcShannonEnt((subDataSet))\n",
    "        #信息增益\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #打印每个特征的信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))\n",
    "        #计算信息增益\n",
    "        if (infoGain > bestInfoGain):\n",
    "            #更新信息增益，找到最大的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            #记录信息增益最大的特征的索引值\n",
    "            bestFeature = i\n",
    "            #返回信息增益最大特征的索引值\n",
    "    return bestFeature\n",
    "\n",
    "\"\"\"\n",
    "函数说明：统计classList中出现次数最多的元素（类标签）\n",
    "Parameters：\n",
    "    classList：类标签列表\n",
    "Returns：\n",
    "    sortedClassCount[0][0]：出现次数最多的元素（类标签）\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    #统计classList中每个元素出现的次数\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "            classCount[vote]+=1\n",
    "        #根据字典的值降序排列\n",
    "        sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "        return sortedClassCount[0][0]\n",
    "\n",
    "\"\"\"\n",
    "函数说明：创建决策树\n",
    "\n",
    "Parameters:\n",
    "    dataSet：训练数据集\n",
    "    labels：分类属性标签\n",
    "    featLabels：存储选择的最优特征标签\n",
    "Returns：\n",
    "    myTree：决策树\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def createTree(dataSet,labels,featLabels):\n",
    "    #取分类标签（是否放贷：yes or no）\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    #如果类别完全相同，则停止继续划分\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    #遍历完所有特征时返回出现次数最多的类标签\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    #选择最优特征\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    #最优特征的标签\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    #根据最优特征的标签生成树\n",
    "    myTree={bestFeatLabel:{}}\n",
    "    #删除已经使用的特征标签\n",
    "    del(labels[bestFeat])\n",
    "    #得到训练集中所有最优特征的属性值\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    #去掉重复的属性值\n",
    "    uniqueVls=set(featValues)\n",
    "    #遍历特征，创建决策树\n",
    "    for value in uniqueVls:\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),\n",
    "                                               labels,featLabels)\n",
    "    return myTree\n",
    "\n",
    "\"\"\"\n",
    "函数说明：获取决策树叶子节点的数目\n",
    "\n",
    "Parameters：\n",
    "    myTree：决策树\n",
    "Returns：\n",
    "    numLeafs：决策树的叶子节点的数目\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def getNumLeafs(myTree):\n",
    "    numLeafs=0\n",
    "    firstStr=next(iter(myTree))\n",
    "    secondDict=myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':\n",
    "            numLeafs+=getNumLeafs(secondDict[key])\n",
    "        else: numLeafs+=1\n",
    "    return numLeafs\n",
    "\n",
    "\"\"\"\n",
    "函数说明:获取决策树的层数\n",
    "\n",
    "Parameters:\n",
    "    myTree:决策树\n",
    "Returns:\n",
    "    maxDepth:决策树的层数\n",
    "\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def getTreeDepth(myTree):\n",
    "    maxDepth = 0                                                #初始化决策树深度\n",
    "    firstStr = next(iter(myTree))                                #python3中myTree.keys()返回的是dict_keys,不在是list,所以不能使用myTree.keys()[0]的方法获取结点属性，可以使用list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]                                #获取下一个字典\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':                #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:   thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth            #更新层数\n",
    "    return maxDepth\n",
    "\n",
    "\"\"\"\n",
    "函数说明:绘制结点\n",
    "\n",
    "Parameters:\n",
    "    nodeTxt - 结点名\n",
    "    centerPt - 文本位置\n",
    "    parentPt - 标注的箭头位置\n",
    "    nodeType - 结点格式\n",
    "Returns:\n",
    "    无\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    arrow_args = dict(arrowstyle=\"<-\")                                            #定义箭头格式\n",
    "    font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)        #设置中文字体\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',    #绘制结点\n",
    "        xytext=centerPt, textcoords='axes fraction',\n",
    "        va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args, FontProperties=font)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:标注有向边属性值\n",
    "\n",
    "Parameters:\n",
    "    cntrPt、parentPt - 用于计算标注位置\n",
    "    txtString - 标注的内容\n",
    "Returns:\n",
    "    无\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]                                            #计算标注位置\n",
    "    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "\n",
    "\"\"\"\n",
    "函数说明:绘制决策树\n",
    "\n",
    "Parameters:\n",
    "    myTree - 决策树(字典)\n",
    "    parentPt - 标注的内容\n",
    "    nodeTxt - 结点名\n",
    "Returns:\n",
    "    无\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def plotTree(myTree, parentPt, nodeTxt):\n",
    "    decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")                                        #设置结点格式\n",
    "    leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")                                            #设置叶结点格式\n",
    "    numLeafs = getNumLeafs(myTree)                                                          #获取决策树叶结点数目，决定了树的宽度\n",
    "    depth = getTreeDepth(myTree)                                                            #获取决策树层数\n",
    "    firstStr = next(iter(myTree))                                                            #下个字典\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)    #中心位置\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)                                                    #标注有向边属性值\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)                                        #绘制结点\n",
    "    secondDict = myTree[firstStr]                                                            #下一个字典，也就是继续绘制子结点\n",
    "    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD                                        #y偏移\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':                                            #测试该结点是否为字典，如果不是字典，代表此结点为叶子结点\n",
    "            plotTree(secondDict[key],cntrPt,str(key))                                        #不是叶结点，递归调用继续绘制\n",
    "        else:                                                                                #如果是叶结点，绘制叶结点，并标注有向边属性值\n",
    "            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n",
    "\n",
    "\"\"\"\n",
    "函数说明:创建绘制面板\n",
    "\n",
    "Parameters:\n",
    "    inTree - 决策树(字典)\n",
    "Returns:\n",
    "    无\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')#创建fig\n",
    "    fig.clf()#清空fig\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)#去掉x、y轴\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))#获取决策树叶结点数目\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))#获取决策树层数\n",
    "    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0#x偏移\n",
    "    plotTree(inTree, (0.5,1.0), '')#绘制决策树\n",
    "    plt.show()#显示绘制结果\n",
    "    \n",
    "\"\"\"\n",
    "使用决策树进行分类\n",
    "Parameters：\n",
    "    inputTree；已经生成的决策树\n",
    "    featLabels：存储选择的最优特征标签\n",
    "    testVec：测试数据列表，顺序对应最优特征标签\n",
    "Returns：\n",
    "    classLabel：分类结果\n",
    "Modify：2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def classify(inputTree,featLabels,testVec):\n",
    "    #获取决策树节点\n",
    "    firstStr=next(iter(inputTree))\n",
    "    #下一个字典\n",
    "    secondDict=inputTree[firstStr]\n",
    "    featIndex=featLabels.index(firstStr)\n",
    "\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex]==key:\n",
    "            if type(secondDict[key]).__name__=='dict':\n",
    "                classLabel=classify(secondDict[key],featLabels,testVec)\n",
    "            else: classLabel=secondDict[key]\n",
    "    return classLabel\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataSet,labels=createDataSet()\n",
    "    featLabels=[]\n",
    "    myTree=createTree(dataSet,labels,featLabels)\n",
    "    print(myTree)\n",
    "    createPlot(myTree)\n",
    "    testVec=[0,1]\n",
    "    result=classify(myTree,featLabels,testVec)\n",
    "\n",
    "    if result=='yes':\n",
    "        print('放贷')\n",
    "    if result=='no':\n",
    "        print('不放贷')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5使用决策树进行分类\n",
    "\n",
    "  依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型。在构建决策树的代码，可以看到，有个featLabels参数。它是用来干什么的？它就是用来记录各个分类结点的，在用决策树做预测的时候，我们按顺序输入需要的分类结点的属性值即可。举个例子，比如我用上述已经训练好的决策树做分类，那么我只需要提供这个人是否有房子，是否有工作这两个信息即可，无需提供冗余的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6决策树的存储\n",
    "\n",
    "    构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\"\"\"\n",
    "函数说明：存储决策树\n",
    "Parameters：\n",
    "    inputTree：已经生成的决策树\n",
    "    filename：决策树的存储文件名\n",
    "Returns：\n",
    "    无\n",
    "Modify：\n",
    "    2018-03-13\n",
    "\n",
    "\"\"\"\n",
    "def storeTree(inputTree,filename):\n",
    "    with open(filename,'wb') as fw:\n",
    "        pickle.dump(inputTree,fw)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    myTree={'有自己的房子':{0:{'有工作':{0:'no',1:'yes'}},1:'yes'}}\n",
    "    storeTree(myTree,'classifierStorage.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "函数说明:读取决策树\n",
    "\n",
    "Parameters:\n",
    "    filename：决策树的存储文件名\n",
    "Returns:\n",
    "    pickle.load(fr)：决策树字典\n",
    "Modify:\n",
    "    2018-03-13\n",
    "\"\"\"\n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    myTree = grabTree('classifierStorage.txt')\n",
    "    print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.7  sklearn——使用决策树预测隐形眼镜类型\n",
    "\n",
    "https://github.com/Jack-Cherish/Machine-Learning/tree/master/Decision%20Tree   参考链接数据集下载\n",
    "\n",
    "步骤：\n",
    "\n",
    "收集数据：使用书中提供的小型数据集\n",
    "\n",
    "准备数据：对文本中的数据进行预处理，如解析数据行\n",
    "\n",
    "分析数据：快速检查数据，并使用createPlot()函数绘制最终的树形图\n",
    "\n",
    "训练决策树：使用createTree()函数训练\n",
    "\n",
    "测试决策树：编写简单的测试函数验证决策树的输出结果&绘图结果\n",
    "\n",
    "使用决策树：这部分可选择将训练好的决策树进行存储，以便随时使用\n",
    "\n",
    "***使用sklearn构建决策树***\n",
    "\n",
    "sklearn.tree——提供了决策树模型，用于解决分类和回归问题   \n",
    "\n",
    "官方链接：http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "参数说明如下：\n",
    "\n",
    "**1.criterion：**特征选择标准，可选参数，默认是gini，可以设置为entropy。gini是基尼不纯度，是将来自集合的某种结果随机应用于某一数据项的预期误差率，是一种基于统计的思想。entropy是香农熵，也就是上篇文章讲过的内容，是一种基于信息论的思想。Sklearn把gini设为默认参数，应该也是做了相应的斟酌的，精度也许更高些？ID3算法使用的是entropy，CART算法使用的则是gini。\n",
    "\n",
    "**2.splitter：**特征划分点选择标准，可选参数，默认是best，可以设置为random。每个结点的选择策略。best参数是根据算法选择最佳的切分特征，例如gini、entropy。random随机的在部分划分点中找局部最优的划分点。默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random”。\n",
    "\n",
    "**3.max_features：**划分时考虑的最大特征数，可选参数，默认是None。寻找最佳切分时考虑的最大特征数(n_features为总共的特征数)，有如下6种情况：\n",
    "\n",
    "    如果max_features是整型的数，则考虑max_features个特征； \n",
    "\n",
    "    如果max_features是浮点型的数，则考虑int(max_features * n_features)个特征；\n",
    "\n",
    "    如果max_features设为auto，那么max_features = sqrt(n_features)；\n",
    "\n",
    "    如果max_features设为sqrt，那么max_featrues = sqrt(n_features)，跟auto一样；\n",
    "\n",
    "    如果max_features设为log2，那么max_features = log2(n_features)；\n",
    "\n",
    "    如果max_features设为None，那么max_features = n_features，也就是所有特征都用。 \n",
    "\n",
    "    一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。\n",
    "    \n",
    "**4.max_depth：**决策树最大深，可选参数，默认是None。这个参数是这是树的层数的。层数的概念就是，比如在贷款的例子中，决策树的层数是2层。如果这个参数设置为None，那么决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。或者如果设置了**min_samples_slipt参数**，那么直到少于min_smaples_split个样本为止。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。\n",
    "\n",
    "**5.min_samples_split**：内部节点再划分所需最小样本数，可选参数，默认是2。这个值限制了子树继续划分的条件。如果min_samples_split为整数，那么在切分内部结点的时候，min_samples_split作为最小的样本数，也就是说，如果样本已经少于min_samples_split个样本，则停止继续切分。如果min_samples_split为浮点数，那么min_samples_split就是一个百分比，ceil(min_samples_split * n_samples)，数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 \n",
    "\n",
    "**6.min_weight_fraction_leaf**叶子节点最小的样本权重和，可选参数，默认是0。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。\n",
    "\n",
    "**7.max_leaf_nodes：**最大叶子节点数，可选参数，默认是None。通过限制最大叶子节点数，可以防止过拟合。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。\n",
    "\n",
    "**8.class_weight：**类别权重，可选参数，默认是None，也可以字典、字典列表、balanced。指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。类别的权重可以通过{class_label：weight}这样的格式给出，这里可以自己指定各个样本的权重，或者用balanced，如果使用balanced，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的None。\n",
    "\n",
    "**9.random_state：**可选参数，默认是None。随机数种子。如果是正数，那么random_state会作为随机数生成器的随机数种子。随机数种子，如果没有设置随机数，随机出来的数与当前系统时间有关，每个时刻都是不同的。如果设置了随机数种子，那么相同随机数种子，不同时刻产生的随机数也是相同的。如果是RandomState instance，那么random_state是随机数生成器。如果为None，则随机数生成器使用np.random。\n",
    "\n",
    "**10.min_impurity_split**：节点划分最小不纯度,可选参数，默认是1e-7。这是个阈值，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。\n",
    "\n",
    "**11.presort：**数据是否预排序，可选参数，默认为False，这个值是布尔值，默认是False不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。 \n",
    "\n",
    "除了这些参数要注意以外，其他在调参时的注意点有：\n",
    "\n",
    "当样本数量少但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型 \n",
    "\n",
    "如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。 \n",
    "\n",
    "推荐多用决策树的可视化，同时先限制决策树的深度，这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。 \n",
    "\n",
    "在训练模型时，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。 \n",
    "\n",
    "决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。 \n",
    "\n",
    "***sklearn.tree.DecisionTreeClassifier()***提供了一些方法供我们使用，如下图所示：\n",
    "![title](images/004.png)\n",
    "\n",
    "**数据预处理**\n",
    "将string类型的数据集进行编码：\n",
    "\n",
    "(1)LabelEncoder：将字符串转换为增量值 \n",
    "\n",
    "(2)OneHotEncoder：使用One-of-K算法将字符串转换为整数\n",
    "\n",
    "为了对string类型的数据序列化，需要先生成pandas数据，这样方便我们的序列化工作。这里我使用的方法是，原始数据->字典->pandas数据，编写代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': ['young', 'young', 'young', 'young', 'young', 'young', 'young', 'young', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'pre', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic', 'presbyopic'], 'prescript': ['myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper', 'myope', 'myope', 'myope', 'myope', 'hyper', 'hyper', 'hyper', 'hyper'], 'astigmatic': ['no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes'], 'tearRate': ['reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal', 'reduced', 'normal']}\n",
      "           age prescript astigmatic tearRate\n",
      "0        young     myope         no  reduced\n",
      "1        young     myope         no   normal\n",
      "2        young     myope        yes  reduced\n",
      "3        young     myope        yes   normal\n",
      "4        young     hyper         no  reduced\n",
      "5        young     hyper         no   normal\n",
      "6        young     hyper        yes  reduced\n",
      "7        young     hyper        yes   normal\n",
      "8          pre     myope         no  reduced\n",
      "9          pre     myope         no   normal\n",
      "10         pre     myope        yes  reduced\n",
      "11         pre     myope        yes   normal\n",
      "12         pre     hyper         no  reduced\n",
      "13         pre     hyper         no   normal\n",
      "14         pre     hyper        yes  reduced\n",
      "15         pre     hyper        yes   normal\n",
      "16  presbyopic     myope         no  reduced\n",
      "17  presbyopic     myope         no   normal\n",
      "18  presbyopic     myope        yes  reduced\n",
      "19  presbyopic     myope        yes   normal\n",
      "20  presbyopic     hyper         no  reduced\n",
      "21  presbyopic     hyper         no   normal\n",
      "22  presbyopic     hyper        yes  reduced\n",
      "23  presbyopic     hyper        yes   normal\n",
      "    age  prescript  astigmatic  tearRate\n",
      "0     2          1           0         1\n",
      "1     2          1           0         0\n",
      "2     2          1           1         1\n",
      "3     2          1           1         0\n",
      "4     2          0           0         1\n",
      "5     2          0           0         0\n",
      "6     2          0           1         1\n",
      "7     2          0           1         0\n",
      "8     0          1           0         1\n",
      "9     0          1           0         0\n",
      "10    0          1           1         1\n",
      "11    0          1           1         0\n",
      "12    0          0           0         1\n",
      "13    0          0           0         0\n",
      "14    0          0           1         1\n",
      "15    0          0           1         0\n",
      "16    1          1           0         1\n",
      "17    1          1           0         0\n",
      "18    1          1           1         1\n",
      "19    1          1           1         0\n",
      "20    1          0           0         1\n",
      "21    1          0           0         0\n",
      "22    1          0           1         1\n",
      "23    1          0           1         0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  # 加载文件\n",
    "    with open('lenses.txt', 'r') as fr:\n",
    "        # 处理文件\n",
    "        lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "    # 提取每组数据的类别，保存在列表里\n",
    "    lenses_target = []\n",
    "    for each in lenses:\n",
    "        lenses_target.append(each[-1])\n",
    "    # 特征标签\n",
    "    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "    # 保存lenses数据的临时列表\n",
    "    lenses_list = []\n",
    "    # 保存lenses数据的字典，用于生成pandas\n",
    "    lenses_dict = {}\n",
    "    # 提取信息，生成字典\n",
    "    for each_label in lensesLabels:\n",
    "        for each in lenses:\n",
    "            lenses_list.append(each[lensesLabels.index(each_label)])\n",
    "        lenses_dict[each_label] = lenses_list\n",
    "        lenses_list = []\n",
    "        # 打印字典信息\n",
    "    print(lenses_dict)\n",
    "    #生成pandas.DataFrame\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)\n",
    "    # print(lenses_pd)\n",
    "    # 生成pandas.DataFrame\n",
    "    lenses_pd = pd.DataFrame(lenses_dict)\n",
    "    # 打印pandas.DataFrame\n",
    "    print(lenses_pd)\n",
    "    # 创建LabelEncoder()对象，用于序列化\n",
    "    le = LabelEncoder()\n",
    "    # 为每一列序列化\n",
    "    for col in lenses_pd.columns:\n",
    "        lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "    print(lenses_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用pydotplus可视化决策树**\n",
    "安装——配置——运行\n",
    "\n",
    "## 3.总结##\n",
    "\n",
    "**1.优点**\n",
    "\n",
    "\n",
    "    易于理解和解释，决策树可以可视化。\n",
    "    \n",
    "    几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。\n",
    "    \n",
    "    使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。\n",
    "\n",
    "    可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。\n",
    "\n",
    "    可以处理多值输出变量问题。\n",
    "\n",
    "    使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。\n",
    "\n",
    "    即使对真实模型来说，假设无效的情况下，也可以较好的适用。\n",
    "\n",
    "**2.缺点**\n",
    "    \n",
    "    决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。\n",
    "    \n",
    "    决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。\n",
    "    \n",
    "    学习一颗最优的决策树是一个NP-完全问题under several aspects of optimality and even for simple concepts。因此，传统决策树算法基于启发式算法，例如贪婪算法，即每个节点创建最优决策。这些算法不能产生一个全家最优的决策树。对样本和特征随机抽样可以降低整体效果偏差。\n",
    "    \n",
    "    概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems.\n",
    "    \n",
    "    如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
