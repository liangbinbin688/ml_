{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn构建朴素贝叶斯分类器\n",
    "\n",
    "在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是**GaussianNB**，**MultinomialNB**和**BernoulliNB**。\n",
    "\n",
    "* GaussianNB就是先验为高斯分布的朴素贝叶斯\n",
    "\n",
    "* MultinomialNB 就是先验为多项式分布的朴素贝叶斯\n",
    "\n",
    "* BernoulliNB 就是先验为伯努利分布的朴素贝叶斯\n",
    "\n",
    "对于新闻的分类，属于多分类问题，可以使用MultinomialNB来完成，假设特征的先验概率为多项式分布：\n",
    "\n",
    "**class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)**\n",
    "\n",
    "* alpha：浮点型可选参数，默认为1.0，其实就是添加拉普拉斯平滑，即为上述公式中的λ ，如果这个参数设置为0，就是不添加平滑；\n",
    "\n",
    "* fit_prior：布尔型可选参数，默认为True。布尔参数fit_prior表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率。否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。\n",
    " \n",
    "* class_prior：可选参数，默认为None。\n",
    "\n",
    "提供的方法：\n",
    "![title](images/006.png)\n",
    "\n",
    "拟合：\n",
    "\n",
    "* fit：一般的拟合\n",
    "\n",
    "* partial_fit：一般用在训练集数据量非常大，一次不能全部载入内存的时候，这个时候可以把训练集分成若干等分，重复调用该方法来一步步学习训练集。\n",
    "\n",
    "预测：\n",
    "\n",
    "* predict：常用的预测方法，直接给出测试集的预测类别输出\n",
    "\n",
    "* predict_log_proba：预测出的各个类别对数概率里的最大值对应的类别，也就是predict方法得到类别\n",
    "\n",
    "* predict_proba：它会给出测试集样本在各个类别上预测的概率，预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到类别。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新浪新闻分类\n",
    "\n",
    "* 第三方分词组件 jeiba    “结巴”  将新闻使用第三方软件进行分词操作\n",
    "\n",
    "* 我们将所有文本分成训练集和测试集，并对训练集中的所有单词进行词频统计，并按降序排序。也就是将出现次数多的词语在前，出现次数少的词语在后进行排序。编写代码如下：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbb\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "def TextProcessing(folder_path,test_size = 0.2):\n",
    "    #查看folder_path下的文件\n",
    "    folder_list=os.listdir(folder_path)\n",
    "    #训练集\n",
    "    data_list=[]\n",
    "    class_list=[]\n",
    "    \n",
    "    #遍历每个子文件夹\n",
    "    for folder in folder_list:\n",
    "        #根据子文件夹，生成新的路径\n",
    "        new_folder_path=os.path.join(folder_path,folder)\n",
    "        #存放子文件夹下的txt文件的列表\n",
    "        files=os.listdir(new_folder_path)\n",
    "\n",
    "        j=1\n",
    "        #遍历每个txt文件\n",
    "        for file in files:\n",
    "            #每类txt样本数最多100个\n",
    "            if j>100:\n",
    "                break\n",
    "            #打开txt文件\n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding='utf-8') as f:\n",
    "                raw=f.read()\n",
    "\n",
    "            #精简模式，返回一个可迭代的generator\n",
    "            word_cut=jieba.cut(raw,cut_all=False)\n",
    "            #generator转换为list\n",
    "            word_list=list(word_cut)\n",
    "\n",
    "            data_list.append(word_list)\n",
    "            class_list.append(folder)\n",
    "            j+=1\n",
    "               #zip压缩合并，将数据与标签对应压缩\n",
    "        data_class_list=list(zip(data_list,class_list))\n",
    "        ## print(\"===========================================================\")\n",
    "        ##print(data_class_list)\n",
    "        #将data_class_list乱序\n",
    "        random.shuffle(data_class_list)\n",
    "        #训练集与测试集切分的索引值\n",
    "        index=int(len(data_class_list)*test_size)+1\n",
    "        #训练集\n",
    "        train_list=data_class_list[index:]\n",
    "        #测试集\n",
    "        test_list=data_class_list[:index]\n",
    "        #训练集解压缩\n",
    "        train_data_list,train_class_list=zip(*train_list)\n",
    "        #测试集解压缩\n",
    "        test_data_list,test_class_list=zip(*test_list)\n",
    "        #统计训练集词频\n",
    "        all_words_dict={}\n",
    "        for word_list in train_data_list:\n",
    "            for word in word_list:\n",
    "                if word in all_words_dict.keys():\n",
    "                    all_words_dict[word]+=1\n",
    "                else:\n",
    "                    all_words_dict[word]=1\n",
    "\n",
    "        #根据键值倒序排列\n",
    "        all_words_tuple_list=sorted(all_words_dict.items(),key=lambda\n",
    "            f:f[1],reverse=True)\n",
    "\n",
    "        #解压缩\n",
    "        all_words_list,all_words_nums=zip(*all_words_tuple_list)\n",
    "        #转换成列表\n",
    "        all_words_list=list(all_words_list)\n",
    "        return all_words_list,train_data_list,test_data_list,train_class_list,\\\n",
    "               test_class_list\n",
    "        #输出的all_word_list就是将所有训练集的切分结果按照词频降序排列构成的单词集合，\n",
    "        #前面包含了很多标点符号，和“是”、“的”、“在”等词语，及数字。所以要将这些去掉。\n",
    "\"\"\"\n",
    "函数说明：读取文件中的内容并去重\n",
    "\n",
    "Parameters：\n",
    "    words_file：文件路径\n",
    "Returns：\n",
    "    word_set：读取内容的set集合\n",
    "Modify：\n",
    "    2018-03-15\n",
    "\n",
    "\"\"\"\n",
    "def MakeWordSet(words_file):\n",
    "    #创建set集合\n",
    "    words_set=set()\n",
    "    #打开文件\n",
    "    with open(words_file,'r',encoding='utf-8') as f:\n",
    "        #一行一行读取\n",
    "        for line in f.readlines():\n",
    "            #去回车\n",
    "            word=line.strip()\n",
    "            #有文本，则添加到word_set中\n",
    "            if len(word)>0:\n",
    "                words_set.add(word)\n",
    "    #返回处理结果\n",
    "    return words_set\n",
    "\"\"\"\n",
    "函数说明：文本特征提取\n",
    "Parameters:\n",
    "    all_words_list - 训练集所有文本列表\n",
    "    deleteN - 删除词频最高的deleteN个词\n",
    "    stopwords_set - 指定的结束语\n",
    "Returns:\n",
    "    feature_words - 特征集\n",
    "Modify:\n",
    "    2018-03-15\n",
    "\"\"\"\n",
    "def words_dict(all_words_list,deleteN,stopWords_set=set()):\n",
    "    #特征列表\n",
    "    feature_words=[]\n",
    "    n=1\n",
    "    for t in range(deleteN,len(all_words_list),1):\n",
    "        #feature_words额维度为1000\n",
    "        if n>1000:\n",
    "            break\n",
    "        #如果这个词不是数字，且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopWords_set \\\n",
    "                and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "        n+=1\n",
    "    return feature_words\n",
    "\n",
    "def TextFeatures(train_data_list, test_data_list, feature_words):\n",
    "    # 出现在特征集中，则置1\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    # 返回结果\n",
    "    return train_feature_list, test_feature_list\n",
    "\n",
    "\"\"\"\n",
    "函数说明：新闻分类器\n",
    "\n",
    "parameters：\n",
    "    train_feature_list - 训练集向量化的特征文本\n",
    "    test_feature_list - 测试集向量化的特征文本\n",
    "    train_class_list - 训练集分类标签\n",
    "    test_class_list - 测试集分类标签\n",
    "Returns:\n",
    "    test_accuracy - 分类器精度\n",
    "Modify:\n",
    "    2018-03-15\n",
    "\"\"\"\n",
    "def TextClassifier(train_feature_list,test_feature_list,train_class_list,test_class_list):\n",
    "    \n",
    "    classifier=MultinomialNB().fit(train_feature_list,train_class_list)\n",
    "    \n",
    "    test_accuracy=classifier.score(test_feature_list,test_class_list)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #文本预处理\n",
    "    #训练集存放地址\n",
    "    folder_path='SogouC\\Sample'\n",
    "    all_words_list, train_data_list, test_data_list, train_class_list, \\\n",
    "    test_class_list=TextProcessing(folder_path,test_size=0.2)\n",
    "    \n",
    "    stopwords_files=\"SogouC/stopwords_cn.txt\"\n",
    "    stopwords_set=MakeWordSet(stopwords_files)\n",
    "    \n",
    "    test_accuracy_list = []\n",
    "    feature_words = words_dict(all_words_list, 450, stopwords_set)\n",
    "    train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)\n",
    "    test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list)\n",
    "    print(test_accuracy)\n",
    "    \n",
    "    #test_accuracy_list.append(test_accuracy)\n",
    "    #ave = lambda c: sum(c) / len(c)\n",
    "    \n",
    "    #print(ave)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    test_accuracy_list=[]\n",
    "    deleteNs=range(0,1000,20)\n",
    "    for deleteN in deleteNs:\n",
    "        feature_words=words_dict(all_words_list,deleteN,stopwords_set)\n",
    "        train_feature_list,test_feature_list=TextFeatures(train_data_list,\n",
    "                                                        test_data_list,feature_words)\n",
    "        test_accuracy=TextClassifier(train_feature_list,test_feature_list,\n",
    "                                     train_class_list,test_class_list)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(deleteNs,test_accuracy_list)\n",
    "    plt.title('Relationship of deleteNs and test_accuracy')\n",
    "    plt.xlabel('deleteNs')\n",
    "    plt.ylabel('test_accuracy')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***总结***\n",
    "\n",
    "* 在训练朴素贝叶斯分类器之前，要处理好训练集，文本的清洗还是有很多需要学习的东西。\n",
    "\n",
    "* 根据提取的分类特征将文本向量化，然后训练朴素贝叶斯分类器。\n",
    "\n",
    "* 去高频词汇数量的不同，对结果也是有影响的的。\n",
    "\n",
    "* v拉普拉斯平滑对于改善朴素贝叶斯分类器的分类效果有着积极的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
