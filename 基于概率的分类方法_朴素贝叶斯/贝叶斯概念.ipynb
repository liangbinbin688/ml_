{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.基于贝叶斯决策理论的方法\n",
    "\n",
    "1.朴素贝叶斯(naive Bayes)算法是基于贝叶斯定理与特征条件独立假设的分类方法，对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概\n",
    "\n",
    "率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y，朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常见的方法。\n",
    "\n",
    "2.朴素贝叶斯(naive Bayes)算法是有监督的学习算法，解决的是**分类问题**，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在\n",
    "\n",
    "于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性\n",
    "\n",
    "假设为前提，就会导致算法精度在某种程度上受影响。\n",
    "\n",
    "## 1.1基于贝叶斯决策理论的分类方法\n",
    "\n",
    "   **优点：** 在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "   \n",
    "   **缺点：** 对于输入数据的准备方式较为敏感\n",
    "   \n",
    "   **适用数据类型：** 标称型数据\n",
    "   \n",
    "### 1.1.1贝叶斯决策理论\n",
    "\n",
    "    朴素贝叶斯是贝叶斯决策理论的一部分，所以首先了解一下贝叶斯理论。\n",
    "    \n",
    "    假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示：\n",
    "    \n",
    "![title](images/001.png)\n",
    "\n",
    "我们现在用$p1(x,y)$表示数据点$(x,y)$属于类别1，用$p2(x,y)$表示数据点$(x,y)$属于类别2，那么对于一个新的数据点$(x,y)$,可以用下面的规则来判断他的类别：\n",
    "    \n",
    "* 如果$p1(x,y)>p2(x,y)$,那么类别为1\n",
    "\n",
    "* 如果$p1(x,y)<p2(x,y)$,那么类别为2\n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。\n",
    "\n",
    "适用决策树不会非常成功，和简单的概率计算相比，KNN计算量太大，因此对于上述问题，最佳选择是概率比较方法。\n",
    "\n",
    "重点是如何计算$p1$和$p2$的概率？\n",
    "\n",
    "### 1.1.2条件概率\n",
    "\n",
    "\n",
    "在学习计算$p1$和$p2$概率之前，我们需要了解什么是条件概率(Condittional probability)，就是指在事件B发生的情况下，事件A发生的概率，用$P(A|B)$来表示。\n",
    "\\bigcup \\bigcap  表示并，交集。\n",
    "\n",
    "$$P(A|B)=\\frac{P({A}\\bigcap{B})}{P(B)}$$\n",
    "\n",
    "故：$$P({A}\\bigcap{B})=P(A|B)P(B)$$\n",
    "\n",
    "同理：$$P({A}\\bigcap{B})=P(B|A)P(A)$$\n",
    "\n",
    "所以：$$P(A|B)P(B)=P(B|A)P(A)$$\n",
    "\n",
    "即：$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "      \n",
    "上式即为条件概率的计算公式.\n",
    "\n",
    "### 1.1.3 全概率公式\n",
    "\n",
    "除了条件概率以外，在计算p1和p2的时候，还要用到全概率公式，因此，这里继续推导全概率公式。\n",
    "\n",
    "$$P(B)=P({B}\\bigcap{A})+P({B}\\bigcap{A^`})$$\n",
    "\n",
    "已知:$$P({B}\\bigcap{A})=P(B|A)P(A)$$\n",
    "\n",
    "故全概率公式为：$$P(B)=P(B|A)P(A)+P(B|A^`)P(A^`)$$\n",
    "\n",
    "其含义是：如果A和A^`构成一个样本空间的一个划分，那么事件B的概率就等于A和A^`的概率分别乘以B对这两个事件的条件概率之和。\n",
    "于是条件概率就有了另一种写法：\n",
    "\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^`)P(A^`)}$$\n",
    "\n",
    "\n",
    "### 1.1.4贝叶斯推断\n",
    "\n",
    "对条件概率进行变形，可以得到如下形式：\n",
    "\n",
    "$$P(A|B)=P(A)\\frac{P(B|A)}{P(B)}$$\n",
    "\n",
    "$$P(类别|特征)=\\frac{P(特征|类别)P(类别)}{P(特征)}$$\n",
    "\n",
    "我们把$P(A)$称为”先验概率”（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。\n",
    "\n",
    "$P(A|B)$称为”后验概率”（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。\n",
    "\n",
    "$\\frac{P(B|A)}{P(B)}$称为“可能性函数”，（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。\n",
    "      \n",
    "所以，条件概率可以理解成下面的式子：\n",
    "\n",
    "$$后验概率=先验概率*调整因子$$\n",
    "\n",
    "这就是贝叶斯推断的含义：我们先预估一个”先验概率”，然后加入实验结果，看这个实验到底是增强还是削弱了”先验概率”，由此得到更接近事实的”后验概率”。\n",
    "\n",
    "在这里，如果”可能性函数”$P(B|A)/P(B)>1$，意味着”先验概率”被增强，事件A的发生的可能性变大；如果”可能性函数”=1，意味着B事件无助于判断事件A的可能性；如果”可能性函数”<1，意味着”先验概率”被削弱，事件A的可能性变小。\n",
    "\n",
    "\n",
    "### 1.4.5朴素贝叶斯\n",
    "\n",
    "“朴素”的解释：假设各个特征之间相互独立（在贝叶斯分类器上做了简化） \n",
    "\n",
    "朴素贝叶斯的基础假设：\n",
    "\n",
    "* 每个特征互相独立；\n",
    "\n",
    "* 每个特征的权重都相等，即对结果的影响程度都相同用。\n",
    "\n",
    "朴素贝叶斯具体实现步骤：\n",
    "\n",
    "![title](images/002.png)\n",
    "\n",
    "一个经典的分类问题：\n",
    "给定数据集：\n",
    "\n",
    "![title](images/003.png)\n",
    "\n",
    "现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？\n",
    "\n",
    "\n",
    "  * 转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！\n",
    "  \n",
    "  这里我们联系到朴素贝叶斯公式：\n",
    "  \n",
    "  ![title](images/004.png)\n",
    "  \n",
    "  我们需要求p(嫁|(不帅、性格不好、身高矮、不上进),这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量.\n",
    "  \n",
    "  p(不帅、性格不好、身高矮、不上进|嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)（至于为什么能求，后面会讲，那么就太好了，将待求的量转化为其\n",
    "  \n",
    "  它可求的值，这就相当于解决了我们的问题！）\n",
    "  \n",
    "  那么这三个量该如何求？\n",
    "  \n",
    "  为什么要假设特征之间互相独立呢？\n",
    "  \n",
    "  * 我们这么想，假如没有这个假设，那么我们对右边这些概率的估计其实是不可做的，这么说，我们这个例子有4个特征，其中帅包括{帅，不帅}，性格包括{不好，好，爆好}，身高包括{高，矮，中}，上进包括{不上进，上进}，那么四个特征的联合概率分布总共是4维空间，总个数为2*3*3*2=36个。36个，计算机扫描统计还可以，但是现实生活中，往往有非常多的特征，每一个特征的取值也是非常之多，那么通过统计来估计后面概率的值，变得几乎不可做，这也是为什么需要假设特征之间独立的原因。\n",
    "  \n",
    "  * 假如我们没有假设特征之间相互独立，那么我们统计的时候，就需要在整个特征空间中去找，比如统计p(不帅、性格不好、身高矮、不上进|嫁),我们就需要在嫁的条件下，去找四种特征全满足分别是不帅，性格不好，身高矮，不上进的人的个数，这样的话，由于数据的稀疏性，很容易统计到0的情况。 这样是不合适的。\n",
    "  \n",
    "  根据上面俩个原因，朴素贝叶斯法对条件概率分布做了条件独立性的假设，由于这是一个较强的假设，朴素贝叶斯也由此得名！这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。\n",
    "  \n",
    "  我们将上面的公式整理一下如下：\n",
    "  \n",
    " ![title](images/005.png)\n",
    " \n",
    "根据样本量计算：$P(嫁)=\\frac{6}{12}$,$P(不帅|嫁)=\\frac{3}{6}$,$P(性格不好|嫁)=\\frac{1}{6}$,$P(矮|嫁)=\\frac{1}{6}$,$P(不上进|嫁)=\\frac{1}{6}$\n",
    "$P(不帅)=\\frac{1}{3}$,$P(性格不好)=\\frac{1}{3}$,$P(身高矮)=\\frac{7}{12}$,$P(不上进)=\\frac{1}{3}$，\n",
    "\n",
    "同样的思路求P(不嫁|不帅，性格不好，身高矮，不上进)，比较最后的结果，嫁与不嫁？\n",
    "\n",
    "## 2.使用朴素贝叶斯进行文档分类\n",
    "    \n",
    "    朴素贝叶斯是用于文档分类的常用的算法。\n",
    "    \n",
    "***朴素贝叶斯的一般过程：***\n",
    "\n",
    "* 收集数据：使用任何方法\n",
    "\n",
    "* 准备数据：需要数值型和布尔型\n",
    "\n",
    "* 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。\n",
    "\n",
    "* 训练算法：计算不同的独立特征的条件概率。\n",
    "\n",
    "* 测试算法：计算错误率\n",
    "\n",
    "* 使用算法：一个常用的朴素贝叶斯应用是文档分类，可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要文本。\n",
    "\n",
    "以在线社区留言为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标志为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型：侮辱类和非侮辱类，使用1和0分别表示\n",
    "\n",
    "我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，再决定将哪些单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。简单起见，我们先假设已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。编写代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "['flea', 'to', 'please', 'food', 'dalmation', 'park', 'worthless', 'problems', 'has', 'dog', 'I', 'posting', 'stop', 'cute', 'help', 'mr', 'so', 'not', 'quit', 'steak', 'how', 'maybe', 'is', 'take', 'love', 'him', 'licks', 'ate', 'stupid', 'garbage', 'my', 'buying']\n",
      "[[1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]]\n",
      "========\n",
      "[0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      "===================\n",
      "8.0\n",
      "========\n",
      "[0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 2. 1. 0. 0.]\n",
      "===================\n",
      "13.0\n",
      "========\n",
      "[0. 1. 0. 1. 0. 1. 2. 0. 0. 2. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 3. 1. 0. 1.]\n",
      "===================\n",
      "19.0\n",
      "++++++++++++++++++++++++++\n",
      "[0.05263158 0.05263158 0.05263158 0.         0.05263158 0.\n",
      " 0.         0.05263158 0.05263158 0.05263158 0.05263158 0.\n",
      " 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.\n",
      " 0.         0.05263158 0.05263158 0.         0.05263158 0.\n",
      " 0.05263158 0.10526316 0.05263158 0.05263158 0.         0.\n",
      " 0.15789474 0.        ] [0.         0.05263158 0.         0.05263158 0.         0.05263158\n",
      " 0.10526316 0.         0.         0.10526316 0.         0.05263158\n",
      " 0.05263158 0.         0.         0.         0.         0.05263158\n",
      " 0.05263158 0.         0.         0.05263158 0.         0.05263158\n",
      " 0.         0.05263158 0.         0.         0.15789474 0.05263158\n",
      " 0.         0.05263158] 0.5\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "[0.         0.05263158 0.         0.05263158 0.         0.05263158\n",
      " 0.10526316 0.         0.         0.10526316 0.         0.05263158\n",
      " 0.05263158 0.         0.         0.         0.         0.05263158\n",
      " 0.05263158 0.         0.         0.05263158 0.         0.05263158\n",
      " 0.         0.05263158 0.         0.         0.15789474 0.05263158\n",
      " 0.         0.05263158]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def loadDataSet():\n",
    "    #切分的词条\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    #类别标签向量，1代表侮辱性词汇，0代表不是\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec\n",
    "\n",
    "\"\"\"\"\n",
    "函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表\n",
    "\n",
    "Parameters：\n",
    "    dataSet：整理的样本数据集\n",
    "Returns：\n",
    "    vocabSet：返回不重复的词条列表，也就是词汇表\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "def createVocabList(dataSet):\n",
    "    #创建一个空的不重复列表\n",
    "    vocabSet=set([])\n",
    "    for document in dataSet:\n",
    "        #取并集\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    return list(vocabSet)\n",
    "\"\"\"\n",
    "函数说明：更加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "\n",
    "Parameters：\n",
    "    vocabList：createVocabList返回的列表\n",
    "    inputSet：切分的词条列表\n",
    "Returns：\n",
    "    returnVec：文档向量，词集模型\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    #创建一个其中所含元素都为0的向量\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    #遍历每个词条\n",
    "    for word in inputSet:\n",
    "        #如果词条存在于词汇表中，则置1\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    #返回向量文档\n",
    "    return returnVec\n",
    "\n",
    "\"\"\"\n",
    "函数说明：朴素贝叶斯分类器训练函数\n",
    "\n",
    "Parameters：\n",
    "    trainMatrix：训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "    trainCategory：训练类别标签向量，即loadDataSet返回的classVec\n",
    "Returns：    \n",
    "    p0Vect：侮辱类的条件概率数组\n",
    "    p1Vect：非侮辱类的条件概率数组\n",
    "    pAbusive：文档属于侮辱类的概率\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def trainNB0(trainMtrix, trainCategory):\n",
    "    # 计算训练的文档数目\n",
    "    numTrainDocs = len(trainMtrix)\n",
    "    # 计算每篇文章的词条数\n",
    "    numWords = len(trainMtrix[0])\n",
    "    # 文档属于侮辱类的概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 创建numpy.zeros数组\n",
    "    p0Num = np.zeros(numWords);\n",
    "    p1Num = np.zeros(numWords)\n",
    "    # 分母初始化为0.0\n",
    "    p0Denom = 0.0;\n",
    "    p1Denom = 0.0\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        # 统计属于侮辱类的条件概率\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMtrix[i]\n",
    "            print(\"========\")\n",
    "            print(p1Num)\n",
    "            p1Denom += sum(trainMtrix[i])\n",
    "            print(\"===================\")\n",
    "            print(p1Denom)\n",
    "        # 统计属于非侮辱类的条件概率\n",
    "        else:\n",
    "            p0Num += trainMtrix[i]\n",
    "            p0Denom += sum(trainMtrix[i])\n",
    "    # 相除\n",
    "    p1Vect = p1Num / p1Denom\n",
    "    p0Vect = p0Num / p1Denom\n",
    "    # 返回属于侮辱类的条件概率\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\"\"\"\n",
    "函数说明:朴素贝叶斯分类器分类函数\n",
    "\n",
    "Parameters:\n",
    "    vec2Classifyaaa：待分类的词条数组\n",
    "    p0Vec：侮辱类的条件概率数组\n",
    "    p1Vec：非侮辱类的条件概率数组\n",
    "    pClass1：文档属于侮辱类的概率\n",
    "Returns:\n",
    "    0 ：属于非侮辱类\n",
    "    1 ：属于侮辱类\n",
    "Modify:\n",
    "    2018-03-14\n",
    "\"\"\"\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    #对应元素相乘\n",
    "    p1=reduce(lambda x,y:x*y,vec2Classify*p1Vec)*pClass1\n",
    "    p0=reduce(lambda x,y:x*y,vec2Classify*p0Vec)*(1.0-pClass1)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    postingList,classVec=loadDataSet()\n",
    "    for each in postingList:\n",
    "        print(each)\n",
    "    print(classVec)\n",
    "    myVocabList=createVocabList(postingList)\n",
    "    print(myVocabList)\n",
    "    trainMat=[]\n",
    "    for postingDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postingDoc))\n",
    "    \n",
    "    print(np.array(trainMat))\n",
    "    #训练朴素贝叶斯分类器\n",
    "    p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(classVec))\n",
    "    print(\"++++++++++++++++++++++++++\")\n",
    "    print(p0V,p1V,pAb)\n",
    "    \n",
    "    #测试样本\n",
    "    testEntry=['love','my','dalmation']\n",
    "    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    #执行分类并打印分类结果\n",
    "    if classifyNB(thisDoc,p0V,p1V,pAb):\n",
    "        print(testEntry,'属于侮辱类')\n",
    "    # 执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry,'属于非侮辱类')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 零概率问题\n",
    "***上述$P0=0.0$,$p1=0.0$,是因为利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$P(w_0|A)P(w_1|A)P(w_3|A)$,如果其中有一个为0，则最后的结果也为0。\n",
    "\n",
    "解决的办法：\n",
    "\n",
    "    为了降低这种影响，可以将所有词的出现次数初始化为1，并将分母初始化为2，这种做法称为“拉普拉斯平滑”，也称“加1平滑”，是比较常用的平滑方法，为了解决0概率问题。\n",
    "\n",
    "### 下溢出\n",
    "\n",
    "    是太多很小的数相乘，越乘越小，就造成了下溢出的问题。在相应小数位置进行四舍五入，计算结果可能就变成0了。\n",
    "    \n",
    "解决办法：\n",
    "\n",
    "    对乘积结果取自然对数，通过求对数可以避免下溢出或者浮点数舍入导致的错误，同时，采用自然对数进行处理不会有任何损失。\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[0, 1, 0, 1, 0, 1]\n",
      "['flea', 'to', 'please', 'food', 'dalmation', 'park', 'worthless', 'problems', 'has', 'dog', 'I', 'posting', 'stop', 'cute', 'help', 'mr', 'so', 'not', 'quit', 'steak', 'how', 'maybe', 'is', 'take', 'love', 'him', 'licks', 'ate', 'stupid', 'garbage', 'my', 'buying']\n",
      "[[1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]]\n",
      "========\n",
      "[1. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 2. 1. 1. 2. 1. 1. 1.]\n",
      "===================\n",
      "10.0\n",
      "========\n",
      "[1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 2. 1. 1. 3. 2. 1. 1.]\n",
      "===================\n",
      "15.0\n",
      "========\n",
      "[1. 2. 1. 2. 1. 2. 3. 1. 1. 3. 1. 2. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2.\n",
      " 1. 2. 1. 1. 4. 2. 1. 2.]\n",
      "===================\n",
      "21.0\n",
      "++++++++++++++++++++++++++\n",
      "[-2.35137526 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -3.04452244\n",
      " -2.35137526 -2.35137526 -2.35137526 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -2.35137526 -1.94591015 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -1.65822808 -3.04452244] [-3.04452244 -2.35137526 -3.04452244 -2.35137526 -3.04452244 -2.35137526\n",
      " -1.94591015 -3.04452244 -3.04452244 -1.94591015 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526 -3.04452244 -3.04452244 -1.65822808 -2.35137526\n",
      " -3.04452244 -2.35137526] 0.5\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "p0: -7.054125771490433\n",
      "p1: -9.826714493730215\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "p0: -6.782192056006791\n",
      "p1: -4.702750514326955\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def loadDataSet():\n",
    "    #切分的词条\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    #类别标签向量，1代表侮辱性词汇，0代表不是\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec\n",
    "\n",
    "\"\"\"\"\n",
    "函数说明：将切分的实验样本词条整理成不重复的词条列表，也就是词汇表\n",
    "\n",
    "Parameters：\n",
    "    dataSet：整理的样本数据集\n",
    "Returns：\n",
    "    vocabSet：返回不重复的词条列表，也就是词汇表\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "def createVocabList(dataSet):\n",
    "    #创建一个空的不重复列表\n",
    "    vocabSet=set([])\n",
    "    for document in dataSet:\n",
    "        #取并集\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    return list(vocabSet)\n",
    "\"\"\"\n",
    "函数说明：更加vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "\n",
    "Parameters：\n",
    "    vocabList：createVocabList返回的列表\n",
    "    inputSet：切分的词条列表\n",
    "Returns：\n",
    "    returnVec：文档向量，词集模型\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    #创建一个其中所含元素都为0的向量\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    #遍历每个词条\n",
    "    for word in inputSet:\n",
    "        #如果词条存在于词汇表中，则置1\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    #返回向量文档\n",
    "    return returnVec\n",
    "\n",
    "\"\"\"\n",
    "函数说明：朴素贝叶斯分类器训练函数\n",
    "\n",
    "Parameters：\n",
    "    trainMatrix：训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "    trainCategory：训练类别标签向量，即loadDataSet返回的classVec\n",
    "Returns：    \n",
    "    p0Vect：侮辱类的条件概率数组\n",
    "    p1Vect：非侮辱类的条件概率数组\n",
    "    pAbusive：文档属于侮辱类的概率\n",
    "Modify：\n",
    "    2018-03-14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def trainNB0(trainMtrix, trainCategory):\n",
    "    # 计算训练的文档数目\n",
    "    numTrainDocs = len(trainMtrix)\n",
    "    # 计算每篇文章的词条数\n",
    "    numWords = len(trainMtrix[0])\n",
    "    # 文档属于侮辱类的概率\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    " # 创建numpy.ones数组，词条初始化为1，拉普拉斯平滑\n",
    "    p0Num = np.ones(numWords);\n",
    "    p1Num = np.ones(numWords)\n",
    "    # 分母初始化为2.0，拉普拉斯平滑\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "\n",
    "    for i in range(numTrainDocs):\n",
    "        # 统计属于侮辱类的条件概率\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMtrix[i]\n",
    "            print(\"========\")\n",
    "            print(p1Num)\n",
    "            p1Denom += sum(trainMtrix[i])\n",
    "            print(\"===================\")\n",
    "            print(p1Denom)\n",
    "        # 统计属于非侮辱类的条件概率\n",
    "        else:\n",
    "            p0Num += trainMtrix[i]\n",
    "            p0Denom += sum(trainMtrix[i])\n",
    "    # 相除\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    p0Vect = np.log(p0Num / p1Denom)\n",
    "    # 返回属于侮辱类的条件概率\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\"\"\"\n",
    "函数说明:朴素贝叶斯分类器分类函数\n",
    "\n",
    "Parameters:\n",
    "    vec2Classifyaaa：待分类的词条数组\n",
    "    p0Vec：侮辱类的条件概率数组\n",
    "    p1Vec：非侮辱类的条件概率数组\n",
    "    pClass1：文档属于侮辱类的概率\n",
    "Returns:\n",
    "    0 ：属于非侮辱类\n",
    "    1 ：属于侮辱类\n",
    "Modify:\n",
    "    2018-03-14\n",
    "\"\"\"\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    #对应元素相乘\n",
    "    #对应元素相乘,logA*B=logA+logB，所以要加上np.log(pClass1)\n",
    "    print(vec2Classify)\n",
    "    p1=sum(vec2Classify*p1Vec)+np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0-pClass1)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    postingList,classVec=loadDataSet()\n",
    "    for each in postingList:\n",
    "        print(each)\n",
    "    print(classVec)\n",
    "    myVocabList=createVocabList(postingList)\n",
    "    print(myVocabList)\n",
    "    trainMat=[]\n",
    "    for postingDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postingDoc))\n",
    "    \n",
    "    print(np.array(trainMat))\n",
    "    #训练朴素贝叶斯分类器\n",
    "    p0V,p1V,pAb=trainNB0(np.array(trainMat),np.array(classVec))\n",
    "    print(\"++++++++++++++++++++++++++\")\n",
    "    print(p0V,p1V,pAb)\n",
    "    #测试样本\n",
    "    testEntry=['love','my','dalmation']\n",
    "    thisDoc=np.array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    #执行分类并打印分类结果\n",
    "    if classifyNB(thisDoc,p0V,p1V,pAb):\n",
    "        print(testEntry,'属于侮辱类')\n",
    "    # 执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry,'属于非侮辱类')\n",
    "    #测试样本2\n",
    "    testEntry=['stupid','garbage']\n",
    "\n",
    "    #测试样本向量化\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    # 执行分类并打印分类结果\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        print(testEntry, '属于侮辱类')\n",
    "        # 执行分类并打印分类结果\n",
    "    else:\n",
    "        print(testEntry, '属于非侮辱类')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
