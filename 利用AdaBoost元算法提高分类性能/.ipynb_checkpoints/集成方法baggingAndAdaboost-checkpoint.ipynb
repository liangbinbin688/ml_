{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.利用AdaBoost元算法提高分类性能\n",
    "\n",
    "    将不同的分类器组合起来的方法被称为集成方法(ensemble method)或者元算法(meta-algorithm)。\n",
    "    \n",
    "    使用集成方法时会有多种形式：可以是不同算法的集成，也可以是同一种算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。\n",
    "\n",
    "##### 集成方法\n",
    "\n",
    "    集成方法（ensemble method）通过组合多个基分类器（base classifier）来完成学习任务，基分类器一般采用的是弱可学习（weakly learnable）分类器，通过集成方法，组合成一个强可学习（strongly learnable）分类器。所谓弱可学习，是指学习的正确率仅略优于随机猜测的多项式学习算法；强可学习指正确率较高的多项式学习算法。集成学习的泛化能力一般比单一的基分类器要好，这是因为大部分集成分类器的分类错误的概率远低于单一基分类器的。\n",
    "    \n",
    "    集成方法主要包括Bagging和Boosting两种方法，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法，即将弱分类器组装成强分类器的方法。\n",
    "    \n",
    "### 1.1bagging（自举汇聚法）\n",
    "    \n",
    "    自举汇聚法（bootstrap aggregating），也称为bagging方法，是在从原始数据集选择S次后得到S个新数据集的一种技术，新数据集和原始数据集大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。替换就意味着可以多次选择同一个样本，即新数据集可以出现重复的值，而原始数据集中的某些值在新集合中则不再出现。\n",
    "    \n",
    "    S个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了S个分类器，当对新数据进行分类时，就可以应用这S个分类器进行分类。与此同时，选择分类器投票结果中最多的类别作为最后的分类结果。\n",
    "    \n",
    "    Bagging对训练数据采用自举采样（boostrap sampling），即有放回地采样数据。\n",
    "    \n",
    "主要思想：\n",
    "\n",
    "+ 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "\n",
    "+ 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "\n",
    "+ 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n",
    "\n",
    "![001](images/001.png)\n",
    "\n",
    "#### 1.1.1随机森林（Random Forest，RF）\n",
    "\n",
    "  随机森林是Bagging的一个变体，在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了**随机属性**选择，具体来说就是传统决策树在选择划分属性时是在当前节点的属性集合（假设有d个属性），中选择一个最优属性，而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最优属性用于划分。\n",
    "  \n",
    "  参数k控制了随机性的引入程度，若令k=d，则基决策树的构建与传统的决策树相同，若令k=1，则是随机选择一个属性用于划分，一般情况下，推荐值$k=log_2{d}$\n",
    "  \n",
    "![001](images/002.png)\n",
    "![001](images/003.png)\n",
    "![001](images/004.png)\n",
    "  \n",
    "随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。\n",
    "\n",
    "随机：先从该节点的属性集合中随机选择一个包含k个属性的子集，然后在从这个子集中选择一个最优属性用于划分。\n",
    "\n",
    "森林：成百上千棵决策树就可以叫做森林\n",
    "\n",
    "直观解释：每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。\n",
    "\n",
    "**特点：**\n",
    "\n",
    "+ 在当前所有算法中，具有极好的准确率\n",
    "+ 能够有效地运行在大数据集上，能够处理具有高维特征的输入样本，而且不需要降维\n",
    "+ 能够评估各个特征在分类问题上的重要性\n",
    "+ 在生成过程中，能够获取到内部生成误差的一种无偏估计\n",
    "+ 对于缺省值问题也能够获得很好得结果\n",
    "\n",
    "随机森林是一种集成学习+决策树的分类模型，它可以利用集成的思想（投票选择的策略）来提升单颗决策树的分类性能（通俗来讲就是“三个臭皮匠，顶一个诸葛亮”）。\n",
    "\n",
    "集集成学习和决策树于一身，随机森林算法具有众多的优点，其中最为重要的就是在随机森林算法中每棵树都尽最大程度的生长，并且没有剪枝过程。\n",
    "\n",
    "随机森林引入了两个随机性——随机选择样本（bootstrap sample）和随机选择特征进行训练。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。\n",
    "\n",
    "### 1.2boosting（提升方法）\n",
    "\n",
    "    提升（boosting）方法是一种与bagging很类似的技术，是常用的统计学习方法，应用广泛且有效。Boosting的思路则是采用重赋权（re-weighting）法迭代地训练基分类器，主要思想：\n",
    "\n",
    "+ 每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果。\n",
    "\n",
    "+ 基分类器之间采用序列式的线性加权方式进行组合。\n",
    "\n",
    "分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。\n",
    "\n",
    "![title](images/005.png)\n",
    "\n",
    "#### Bagging、Boosting二者之间的区别\n",
    "\n",
    "+ 样本选择上：\n",
    "    - Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\n",
    "    \n",
    "    - Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n",
    "    \n",
    "+ 样例权重：\n",
    "    - Bagging：使用均匀取样，每个样例的权重相等。\n",
    "    \n",
    "    - Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n",
    "    \n",
    "+ 预测函数：\n",
    "    - Bagging：所有预测函数的权重相等。\n",
    "    \n",
    "    - Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n",
    "    \n",
    "+ 并行计算：\n",
    "\n",
    "    - Bagging：各个预测函数可以并行生成。\n",
    "    \n",
    "    - Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n",
    "    \n",
    " 总结：\n",
    " \n",
    " 这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n",
    " \n",
    " + Bagging + 决策树 = 随机森林\n",
    " \n",
    " + AdaBoost + 决策树 = 提升树\n",
    " \n",
    " + Gradient Boosting + 决策树 = GBDT\n",
    " \n",
    " ### 1.3 Adaboost\n",
    "         \n",
    "![title](images/006.png)\n",
    "\n",
    "    AdaBoost训练方法：基于错误提示分类器的性能\n",
    "    \n",
    "![title](images/007.png) \n",
    "\n",
    "![title](images/008.png)\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "+ 计算样本权重\n",
    "    训练数据中的每个样本，赋予其权重，即样本权重，用向量D表示，这些权重都初始化成相等值。假设有n个样本的训练集：$$\\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_n,y_n)\\}$$.\n",
    "+ 计算错误率\n",
    "    利用第一个弱学习算法h1对其进行学习，学习完成后进行错误率ε的统计：$$e=\\frac{未正确分类的样本数量}{所有样本数}$$\n",
    "\n",
    "+ 计算弱学习算法权重\n",
    "    弱学习算法也有一个权重，用向量α表示，利用错误率计算权重α： $${\\alpha}=\\frac{1}{2}ln({\\frac{1-e}{e}})$$\n",
    "    \n",
    "+ 更新样本权重\n",
    "    在第一次学习完成后，需要重新调整样本的权重，以使得在第一分类中被错分的样本的权重，在接下来的学习中可以重点对其进行学习： \n",
    "    \n",
    "![title](images/009.png)\n",
    "\n",
    "其中$Z_t$是一个归一化的因子：\n",
    "![title](images/010.png)\n",
    "\n",
    "+ 重复进行学习\n",
    "\n",
    "重复进行学习，这样经过t轮的学习后，就会得到t个弱学习算法、权重、弱分类器的输出以及最终的AdaBoost算法的输出，分别如下： \n",
    "\n",
    "![title](images/011.png)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn的AdaBoost\n",
    "\n",
    "***class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)***\n",
    "\n",
    "\n",
    "+ base_estimator：可选参数，默认为DecisionTreeClassifier。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。\n",
    "\n",
    "+ algorithm：可选参数，默认为SAMME.R。scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "+ n_estimators：整数型，可选参数，默认为50。弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "+ learning_rate：浮点型，可选参数，默认为1.0。每个弱学习器的权重缩减系数，取值范围为0到1，对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1。\n",
    "\n",
    "+ random_state：整数型，可选参数，默认为None。如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-23e4a28ff001>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-23e4a28ff001>\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    predictions = bdt.predict(d ataArr)\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#病马数据集，将标签变为+1/-1\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len((open(fileName).readline().split('\\t')))\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat - 1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat, labelMat\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataArr, classLabels = loadDataSet('horseColicTraining2.txt')\n",
    "    testArr, testLabelArr = loadDataSet('horseColicTest2.txt')\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), algorithm='SAMME', n_estimators=10)\n",
    "    bdt.fit(dataArr, classLabels)\n",
    "    predictions = bdt.predict(dataArr)\n",
    "    errArr = np.mat(np.ones((len(dataArr), 1)))\n",
    "    print('训练集错误率：%.3f%%' % float(errArr[predictions != classLabels].sum() / len(dataArr) * 100))\n",
    "    predictions = bdt.predict(testArr)\n",
    "    errArr = np.mat(np.ones((len(testArr), 1)))\n",
    "    print('测试集错误率：%.3f%%' % float(errArr[predictions != testLabelArr].sum() /len(testArr)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
