{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.SVM概念和基础\n",
    "\n",
    "    分类作为数据挖掘领域中一项非常重要的任务，它的目的是学会一个分类函数或分类模型(或者叫做分类器)，而支持向量机本身便是一种监督式学习的方法，它广泛的应用于统计分类以及回归分析中。\n",
    "    \n",
    "    支持向量机（SVM）是90年代中期发展起来的基于统计学习理论的一种机器学习方法，通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。\n",
    "    \n",
    "    通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。\n",
    "\n",
    "###   1.1线性可分的二分类问题：\n",
    "![title](images/001.png)\n",
    "    \n",
    "  上图中红色和蓝色分别表示不同的两个类别，数据为线性可分，但是将其分开的直线不止一条，(b)(c)分别给出了不同的方法。黑色的实现为“决策面”，每个决策面对应一个线性分类器，两者的性能是有差距的。 \n",
    "![title](images/002.png)\n",
    "\n",
    " 决策面不同的情况下，添加一个红色的点，显然(b)仍然能够很好的分类，但是(c)已经分类错误了，所以决策面(b)优于(c)。\n",
    " \n",
    " 在保证决策面方向不变且不会出现错分样本的情况下移动决策面，会在原来的决策面两侧找到两个极限位置（越过该位置就会产生错分现象），如虚线所示。\n",
    " \n",
    " 虚线的位置由决策面的方向和距离原决策面最近的几个样本的位置决定。而这两条平行虚线正中间的分界线就是在保持当前决策面方向不变的前提下的最优决策面。\n",
    " \n",
    " **两条虚线之间的垂直距离就是这个最优决策面对应的分类间隔**。显然每一个可能把数据集正确分开的方向都有一个最优决策面（有些方向无论如何移动决策面的位置也不可能将两类样本完全分开），而不同方向的最优决策面的分类间隔通常是不同的，那个具有“最大间隔”的决策面就是SVM要寻找的最优解。\n",
    " \n",
    " 而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的支持样本点，称为”支持向量”。\n",
    "学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类中。\n",
    "\n",
    "分割超平面的方程为:${\\omega}x+{b}=0$\n",
    "\n",
    "方程由方程由${\\omega}$,$b$决定，用$({\\omega},b)$表示。\n",
    "\n",
    "![title](images/003.png)\n",
    "\n",
    "![title](images/004.png)\n",
    "\n",
    "sign(x)或者Sign(x)叫做符号函数,当x>0时，sign(x)=1;当x=0时，sign(x)=0;当x<0时，sign(x)=-1。\n",
    "\n",
    "### 1.2几何间隔和函数间隔\n",
    "\n",
    "![title](images/005.png)\n",
    "\n",
    "![title](images/006.png)\n",
    "\n",
    "![title](images/007.png)\n",
    "\n",
    "#### 函数间隔：\n",
    "\n",
    "    一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度，在超平面${\\omega}*x=+b=0$确定的情况下，$|{\\omega}*x+b|$能够相对的表示点x到超平面的距离的远近，而${\\omega}*+b$的符号与类标记y的符号是否一致能够表示分类是否正确，所以可以量$y({\\omega}*x+b)$来表示分离的正确度和确信度。\n",
    "    \n",
    "![title](images/008.png)    \n",
    "\n",
    "#### 从函数间隔变为几何间隔：\n",
    "\n",
    "  虽然函数间隔可以表示分类预测的正确性即确信度，但是选择分离超平面时，只有函数间隔远远不够，因为只要成比例的改变${\\omega}$,$b$.例如将他们改编成$2{\\omega}$,$2b$，超平面并没有改变，但是函数间隔$(y({\\omega}*x+b))$ 却成为了原来的2倍。\n",
    "  \n",
    "  \n",
    "  对分离超平面的法向量${\\omega}$加以约束，如规范化，$||{\\omega}||=1$,使得间隔是确定的，此时函数间隔变为几何间隔。\n",
    "  \n",
    "![title](images/009.png)\n",
    "\n",
    "上图给出了超平面$({\\omega},b)$,和其法向量${\\omega}$,点A表示某一个实例$x_i$,其类标记为:$y_i=+1$,点A与超平面的距离由线段AB给出，记作$y_i$:\n",
    "\n",
    "$$y_i=\\frac{\\omega}{||{\\omega}||}*x_i+\\frac{b}{\\omega}$$.\n",
    "\n",
    "当点A在超平面的负一侧时，即$y_i=-1$时，\n",
    "\n",
    "$$y_i=-{\\frac{\\omega}{||{\\omega}||}*x_i+\\frac{b}{\\omega}}$$.\n",
    "\n",
    "一般情况下:当样本点$(x_i,y_i)$被超平面$({\\omega},b)$正确分类时，点$x_i$到超平面的距离时：\n",
    "\n",
    "$${\\gamma}_i=y_i(\\frac{\\omega}{||{\\omega}||}*x_i+\\frac{b}{\\omega})$$\n",
    "    \n",
    "![title](images/010.png)\n",
    "\n",
    "\n",
    "#### 最大间隔分离超平面\n",
    "\n",
    "![title](images/011.png)\n",
    "\n",
    "最大间隔分离超平面，即为下面的约束最优化问题：\n",
    "\n",
    "![title](images/012.png)\n",
    "\n",
    "**其中加入大于等于1是为了计算标签方便，不管是正负样本，只要正确分类，都是大于等于0的。**a\n",
    "\n",
    "**线性可分训练数据集的最大间隔分离超平面是存在且唯一的.**\n",
    "\n",
    "\n",
    "#### 支持向量和间隔边界\n",
    "\n",
    "在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量，支持向量是使约束条件等号成立的点，即： \n",
    "\n",
    "$$y_i({\\omega}_ix_i+b)-1=0$$\n",
    "\n",
    "![title](images/013.png)\n",
    "\n",
    "如图所示，$H_1$,$H_2$都是支持向量积上的点。\n",
    "\n",
    "间隔边界：\n",
    "\n",
    "$H_1$,$H_2$平行于分离超平面，他们之间没有其他的样本点，间隔距离为$\\frac{2}{||{\\omega}||}$,他们称为间隔边界。\n",
    "\n",
    "由于支持向量在确定分离超平面中起着决定性作用，所以将这种分离模型称为支持向量机。\n",
    "支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。\n",
    "\n",
    "![title](images/014.png)\n",
    "\n",
    "\n",
    "### 1.3学习的对偶算法（最优化问题）\n",
    "\n",
    "为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。\n",
    "\n",
    "对偶算法的优点：\n",
    "\n",
    "+ 对偶问题往往更容易求解 \n",
    "\n",
    "+ 自然引入核函数，进而推广到非线性分类问题\n",
    "\n",
    "首先，我们先要从宏观的视野上了解一下拉格朗日对偶问题出现的原因和背景。\n",
    "\n",
    "我们知道我们要求解的是最小化问题，所以一个直观的想法是如果我能够构造一个函数，使得该函数在可行解区域内与原目标函数完全一致，而在可行解区域外的数值非常大，甚至是无穷大，那么这个没有约束条件的新目标函数的优化问题就与原来有约束条件的原始目标函数的优化问题是等价的问题。这就是使用拉格朗日方程的目的，它将约束条件放到目标函数中，从而将有约束优化问题转换为无约束优化问题。\n",
    "\n",
    "随后，人们又发现，使用拉格朗日获得的函数，使用求导的方法求解依然困难。进而，需要对问题再进行一次转换，即使用一个数学技巧：拉格朗日对偶。\n",
    "\n",
    "所以，显而易见的是，我们在拉格朗日优化我们的问题这个道路上，需要进行下面二个步骤：\n",
    "\n",
    "+ 1.将有约束的原始目标函数转换为无约束的新构造的拉格朗日目标函数\n",
    "\n",
    "+ 2.使用拉格朗日对偶性，将不易求解的优化问题转化为易求解的优化\n",
    "\n",
    "而求解这个对偶学习问题，可以分为三个步骤：首先要让$L({\\omega},b,{\\alpha})$关于${\\omega}$,$b$最小化，然后求对${\\alpha}$的最大化，最后利用SMO算法求解对偶问题中的拉格朗日乘子。\n",
    "\n",
    "![title](images/018.png)\n",
    "\n",
    "![title](images/015.png)\n",
    "\n",
    "![title](images/016.png)\n",
    "\n",
    "![title](images/017.png)\n",
    "    \n",
    "KKT条件：\n",
    "\n",
    "假设一个最优化模型能够表示成下列标准形式：\n",
    "\n",
    "![title](images/019.png)\n",
    "\n",
    "KKT条件的全称是Karush-Kuhn-Tucker条件，KKT条件是说最优值条件必须满足以下条件： \n",
    "\n",
    "+ 条件一：经过拉格朗日函数处理之后的新目标函数L(w,b,α)对α求导为零\n",
    "\n",
    "+ 条件二：$h(x)=0$;\n",
    "\n",
    "+ 条件三：${\\alpha}*g(x)=0$\n",
    "\n",
    "\n",
    "对于我们的优化问题：\n",
    "![titlr](images/020.png)\n",
    "\n",
    "上述优化问题满足三个条件，故满足了凸优化问题和KKT条件。\n",
    "![titlr](images/021.png)\n",
    "\n",
    "![titlr](images/022.png)\n",
    "\n",
    "![titlr](images/023.png)\n",
    "\n",
    "### 1.4线性支持向量机与软间隔最大化\n",
    "\n",
    "    线性可分问题的支持向量机学习方法对线性不可分训练数据是不适用的，因为上述方法在那个的不等式约束并不成立。\n",
    "    \n",
    "    如何将其扩展到线性不可分问题：\n",
    "    \n",
    "    1.修改硬间隔最大化，使其成为软间隔最大化。\n",
    "    \n",
    "    假设给定一个特征空间上的训练数据集：\n",
    "    \n",
    "    假设训练数据集是线性不可分的，通常情况是，训练数据中有一些特异点（outlier），将这些特异点去除后，剩下的大部分样本点组成的几何是线性可分的。\n",
    "    \n",
    "![title](images/024.png)\n",
    "\n",
    "\n",
    "+ 松弛因子：函数间隔加上松弛因子满足大于等于零，使不满足分割条件的样本满足分割条件，就是软间隔最大化的主要思想。\n",
    "\n",
    "+ 惩罚参数：权衡间距最大化和错分类样本个数的系数，值越大说明对错分类样本的惩罚越大。\n",
    "\n",
    "  求解${\\alpha}$的思路跟线性可分一样,然后求${\\omega}$的值，最后得到超平面函数，分类器函数。\n",
    "\n",
    "#### 学习的对偶算法(线性不可分)\n",
    "\n",
    "![title](images/025.png)\n",
    "\n",
    "![title](images/026.png)\n",
    "\n",
    "![title](images/027.png)\n",
    "\n",
    "![title](images/028.png)\n",
    "\n",
    "支持向量：\n",
    "\n",
    "![title](images/029.png)\n",
    "\n",
    "合页损失函数\n",
    "![title](images/030.png)\n",
    "![title](images/031.png)\n",
    "![title](images/032.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '简化版SMO算法' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1d2f8175d12e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#SMO算法实现SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m简化版SMO算法\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '简化版SMO算法' is not defined"
     ]
    }
   ],
   "source": [
    "#SMO算法实现SVM\n",
    "#简化版SMO算法\n",
    "from time import sleep\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import types\n",
    "\n",
    "\"\"\"\n",
    "函数说明：读取数据\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():  # 逐行读取，滤除空格等\n",
    "        lineArr = line.strip().split('\\t')\n",
    "        dataMat.append([float(lineArr[0]), float(lineArr[1])])  # 添加数据\n",
    "        labelMat.append(float(lineArr[2]))  # 添加标签\n",
    "    return dataMat, labelMat\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：随机选择alpha\n",
    "\n",
    "Parameters:\n",
    "    i：alpha\n",
    "    m：alpha参数个数\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def selectJrand(i, m):\n",
    "    j = i\n",
    "    # 选择一个不等于i的j\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0, m))\n",
    "    return j\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：修剪alpha\n",
    "Parameters：\n",
    "    aj:alpha的值\n",
    "    H：alpha上限\n",
    "    L：alpha下限\n",
    "\n",
    "Returns：\n",
    "    aj：alpha的值\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clipAlpha(aj, H, L):\n",
    "    if aj > H:\n",
    "        aj = H\n",
    "    if L > aj:\n",
    "        aj = L\n",
    "    return aj\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：简化版SMO算法\n",
    "\n",
    "Parameters：\n",
    "    dataMatIn：数据矩阵\n",
    "    classLabels：数据标签\n",
    "    C：松弛变量\n",
    "    toler：容错率\n",
    "    maxIter：最大迭代次数\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    # 转换为numpy的mat存储\n",
    "    dataMatrix = np.mat(dataMatIn)\n",
    "    labelMat = np.mat(classLabels).transpose()\n",
    "    # 初始化b参数，统计dataMatrix的维度\n",
    "    b = 0;\n",
    "    m, n = np.shape(dataMatrix)\n",
    "    # 初始化alpha参数，设为0\n",
    "    alphas = np.mat(np.zeros((m, 1)))\n",
    "    # 初始化迭代次数\n",
    "    iter_num = 0\n",
    "    # 最多迭代matIter次\n",
    "    while (iter_num < maxIter):\n",
    "        alphaPairsChanged = 0\n",
    "        for i in range(m):\n",
    "            # 步骤1：计算误差Ei\n",
    "            fXi = float(np.multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[i, :].T)) + b\n",
    "            Ei = fXi - float(labelMat[i])\n",
    "            # 优化alpha，设定一定的容错率。\n",
    "            if ((labelMat[i] * Ei < -toler) and (alphas[i] < C)) or ((labelMat[i] * Ei > toler) and (alphas[i] > 0)):\n",
    "                # 随机选择另一个与alpha_i成对优化的alpha_j\n",
    "                j = selectJrand(i, m)\n",
    "                # 步骤1：计算误差Ej\n",
    "                fXj = float(np.multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[j, :].T)) + b\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                # 保存更新前的aplpha值，使用深拷贝\n",
    "                alphaIold = alphas[i].copy()\n",
    "                alphaJold = alphas[j].copy()\n",
    "                # 步骤2：计算上下界L和H\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                if L == H: print(\"L==H\"); continue\n",
    "                # 步骤3：计算eta\n",
    "                eta = 2.0 * dataMatrix[i, :] * dataMatrix[j, :].T - dataMatrix[i, :] * dataMatrix[i, :].T -\\\n",
    "                      dataMatrix[j,:] * dataMatrix[j, :].T\n",
    "                if eta >= 0: print(\"eta>=0\"); continue\n",
    "                # 步骤4：更新alpha_j\n",
    "                alphas[j] -= labelMat[j] * (Ei - Ej) / eta\n",
    "                # 步骤5：修剪alpha_j\n",
    "                alphas[j] = clipAlpha(alphas[j], H, L)\n",
    "                if (abs(alphas[j] - alphaJold) < 0.00001): print(\"alpha_j变化太小\"); continue\n",
    "                # 步骤6：更新alpha_i\n",
    "                alphas[i] += labelMat[j] * labelMat[i] * (alphaJold - alphas[j])\n",
    "                # 步骤7：更新b_1和b_2\n",
    "                b1 = b - Ei - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[i, :].T - labelMat[\n",
    "                    j] * (alphas[j] - alphaJold) * dataMatrix[i, :] * dataMatrix[j, :].T\n",
    "                b2 = b - Ej - labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[j, :].T - labelMat[\n",
    "                    j] * (alphas[j] - alphaJold) * dataMatrix[j, :] * dataMatrix[j, :].T\n",
    "                # 步骤8：根据b_1和b_2更新b\n",
    "                if (0 < alphas[i]) and (C > alphas[i]):\n",
    "                    b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]):\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2) / 2.0\n",
    "                # 统计优化次数\n",
    "                alphaPairsChanged += 1\n",
    "                # 打印统计信息\n",
    "                print(\"第%d次迭代 样本:%d, alpha优化次数:%d\" % (iter_num, i, alphaPairsChanged))\n",
    "        # 更新迭代次数\n",
    "        if (alphaPairsChanged == 0):\n",
    "            iter_num += 1\n",
    "        else:\n",
    "            iter_num = 0\n",
    "        print(\"迭代次数: %d\" % iter_num)\n",
    "    return b, alphas\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：分类结果可视化\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def showClassifer(dataMat, w, b):\n",
    "    # 绘制样本点\n",
    "    data_plus = []  # 正样本\n",
    "    data_minus = []  # 负样本\n",
    "    for i in range(len(dataMat)):\n",
    "        if labelMat[i] > 0:\n",
    "            data_plus.append(dataMat[i])\n",
    "        else:\n",
    "            data_minus.append(dataMat[i])\n",
    "    data_plus_np = np.array(data_plus)  # 转换为numpy矩阵\n",
    "    data_minus_np = np.array(data_minus)  # 转换为numpy矩阵\n",
    "    plt.scatter(np.transpose(data_plus_np)[0], np.transpose(data_plus_np)[1], s=30,alpha=0.7)  # 正样本散点图\n",
    "    plt.scatter(np.transpose(data_minus_np)[0], np.transpose(data_minus_np)[1], s=30,alpha=0.7)  # 负样本散点图\n",
    "    # 绘制直线\n",
    "    x1 = max(dataMat)[0]\n",
    "    x2 = min(dataMat)[0]\n",
    "    a1, a2 = w\n",
    "    b = float(b)\n",
    "    a1 = float(a1[0])\n",
    "    a2 = float(a2[0])\n",
    "    y1, y2 = (-b - a1 * x1) / a2, (-b - a1 * x2) / a2\n",
    "    plt.plot([x1, x2], [y1, y2])\n",
    "    # 找出支持向量点\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if abs(alpha) > 0:\n",
    "            x, y = dataMat[i]\n",
    "            plt.scatter([x], [y], s=150, c='none', alpha=0.7, linewidth=1.5, edgecolor='red')\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "函数说明：计算w\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_w(dataMat, labelMat, alphas):\n",
    "    alphas, dataMat, labelMat = np.array(alphas), np.array(dataMat), np.array(labelMat)\n",
    "    w = np.dot((np.tile(labelMat.reshape(1, -1).T, (1, 2)) * dataMat).T, alphas)\n",
    "    return w.tolist()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataMat, labelMat = loadDataSet('testSet.txt')\n",
    "    b, alphas = smoSimple(dataMat, labelMat, 0.6, 0.001, 40)\n",
    "    w = get_w(dataMat, labelMat, alphas)\n",
    "    showClassifer(dataMat, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
